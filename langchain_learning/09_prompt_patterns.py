import os
from dotenv import load_dotenv
from langchain_core.prompts import (
    ChatPromptTemplate,
    FewShotChatMessagePromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain_openai import ChatOpenAI

# 1. Load Environment Variables
load_dotenv()
if not os.getenv("DEEPSEEK_API_KEY"):
    print("âš ï¸ Please set DEEPSEEK_API_KEY in .env file")
    exit(1)

llm = ChatOpenAI(
    model="deepseek-chat", 
    temperature=0,
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url=os.getenv("DEEPSEEK_API_BASE")
)

def demo_system_prompt_best_practices():
    print("\n--- 1. System Prompt Best Practices ---")
    # Pattern: Role + Context + Constraints + Output Format
    system_template = (
        "ä½ æ˜¯ä¸€ä½ç²¾é€š Python æ–‡æ¡£çš„èµ„æ·±æŠ€æœ¯ä½œå®¶ã€‚\n"
        "ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·æ‚ä¹±çš„ä»£ç æ³¨é‡Šé‡å†™ä¸ºä¸“ä¸šçš„ Docstringsã€‚\n"
        "çº¦æŸæ¡ä»¶ï¼š\n"
        "- ä½¿ç”¨ Google é£æ ¼çš„ Python Docstringsã€‚\n"
        "- ä¸è¦ä¿®æ”¹ä»£ç é€»è¾‘ã€‚\n"
        "- ä¿æŒç®€æ´ã€‚"
    )
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", "{code_snippet}"),
    ])
    
    code = "def add(a,b): # adds two numbers and returns result\n    return a+b"
    
    chain = prompt | llm
    result = chain.invoke({"code_snippet": code})
    print(f"Input Code:\n{code}")
    print(f"Output Docstring:\n{result.content}")

def demo_few_shot_prompting():
    print("\n--- 2. Few-Shot Prompting (Structured) ---")
    # Use Few-Shot to teach the model a specific tone or format that is hard to describe.
    
    # 1. Define examples
    examples = [
        {"input": "ä»Šå¤©å¤©æ°”çœŸå¥½ã€‚", "output": "æƒ…æ„Ÿï¼šæ­£é¢ | Emojiï¼šâ˜€ï¸"},
        {"input": "æˆ‘è¢«å µåœ¨è·¯ä¸Šäº†ã€‚", "output": "æƒ…æ„Ÿï¼šè´Ÿé¢ | Emojiï¼šğŸš—"},
        {"input": "æˆ‘ä¸çŸ¥é“åƒä»€ä¹ˆã€‚", "output": "æƒ…æ„Ÿï¼šä¸­æ€§ | Emojiï¼šğŸ½ï¸"},
    ]
    
    # 2. Define a prompt template for the examples
    example_prompt = ChatPromptTemplate.from_messages([
        ("human", "{input}"),
        ("ai", "{output}"),
    ])
    
    # 3. Create the FewShot template
    few_shot_prompt = FewShotChatMessagePromptTemplate(
        example_prompt=example_prompt,
        examples=examples,
    )
    
    # 4. Combine with final prompt
    final_prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†ææœºå™¨äººã€‚è¯·æŒ‰æ ¼å¼è¾“å‡ºï¼šæƒ…æ„Ÿï¼šX | Emojiï¼šY"),
        few_shot_prompt,
        ("human", "{user_input}"),
    ])
    
    chain = final_prompt | llm
    user_input = "æˆ‘çš„ä»£ç ç»ˆäºè·‘é€šäº†ï¼"
    result = chain.invoke({"user_input": user_input})
    print(f"User: {user_input}")
    print(f"Agent: {result.content}")

def demo_chain_of_thought():
    print("\n--- 3. Chain of Thought (CoT) ---")
    # Explicitly asking the model to "think step by step" to improve logic.
    
    question = "å¦‚æœæˆ‘æœ‰3ä¸ªè‹¹æœï¼Œåƒæ‰äº†1ä¸ªï¼Œåˆä¹°äº†5ä¸ªï¼Œç„¶åæŠŠæ€»æ•°çš„ä¸€åŠç»™æœ‹å‹ï¼Œæˆ‘è¿˜å‰©å¤šå°‘ä¸ªï¼Ÿ"
    
    # Without CoT (sometimes fails on complex logic, though simple math is usually fine)
    # With CoT (explicit instruction)
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªé€»è¾‘ä¸¥å¯†çš„æ•°å­¦åŠ©æ‰‹ã€‚"),
        ("human", "{question}\nè¯·ä¸€æ­¥æ­¥æ€è€ƒã€‚"),
    ])
    
    chain = prompt | llm
    result = chain.invoke({"question": question})
    print(f"Question: {question}")
    print(f"Answer (Step-by-Step):\n{result.content}")

if __name__ == "__main__":
    demo_system_prompt_best_practices()
    demo_few_shot_prompting()
    demo_chain_of_thought()
